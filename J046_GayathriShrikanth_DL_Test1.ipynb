{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J046_GayathriShrikanth_DL_Test1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbkGHBYX0ltgbpIC2Uc18E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GayathriShrikanth/DeepLearning/blob/master/J046_GayathriShrikanth_DL_Test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh9kWHjKACHV",
        "colab_type": "text"
      },
      "source": [
        "# **Gayathri Shrikanth J046**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW3hLUNxuDa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_QxVY2uQ49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_data = load_iris()\n",
        "x = iris_data.data\n",
        "y_ = iris_data.target.reshape(-1, 1) \n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdrUre8RuQ8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data for training and testing\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.01)\n",
        "train_x, X_val, train_y, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT5xHURXxpar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_acc={}\n",
        "final_loss={}\n",
        "cv_acc={}\n",
        "cv_loss={}\n",
        "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "rms=keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "sgd=keras.optimizers.SGD(lr=0.001, momentum=0.0, nesterov=False)\n",
        "ada=keras.optimizers.Adagrad(lr=0.01)\n",
        "adelta=keras.optimizers.Adadelta(lr=1.0, rho=0.95)\n",
        "adamax=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
        "nadam=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
        "opt=[adam, rms, sgd, ada, adelta, adamax, nadam]\n",
        "opt_name=[\"adam\", \"rms\", \"sgd\", \"ada\", \"adelta\", \"adamax\", \"nadam\"]\n",
        "n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQSl4SquQ2a",
        "colab_type": "code",
        "outputId": "354537ab-53a9-4620-c14d-ba33da8e6fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for optim in opt:\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  # Train the model\n",
        "  model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=100, validation_data=(test_x, test_y))\n",
        "  # Test on unseen data\n",
        "  results = model.evaluate(test_x, test_y)\n",
        "  final_acc[opt_name[n]]=results[1]\n",
        "  final_loss[opt_name[n]]=results[0]\n",
        "  results_cv = model.evaluate(X_val, y_val)\n",
        "  cv_acc[opt_name[n]]=results_cv[1]\n",
        "  cv_loss[opt_name[n]]=results_cv[0]\n",
        "  n=n+1\n",
        "  print('Final test set loss: {:4f}'.format(results[0]))\n",
        "  print('Final test set accuracy: {:4f}'.format(results[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.0604 - acc: 0.3220 - val_loss: 0.7764 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.0147 - acc: 0.3305 - val_loss: 0.7830 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.9888 - acc: 0.3898 - val_loss: 0.7874 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9668 - acc: 0.4322 - val_loss: 0.7850 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.9466 - acc: 0.4322 - val_loss: 0.7843 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.9258 - acc: 0.4322 - val_loss: 0.7690 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.9071 - acc: 0.4407 - val_loss: 0.7620 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.8854 - acc: 0.4661 - val_loss: 0.7465 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.8656 - acc: 0.4746 - val_loss: 0.7348 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.8472 - acc: 0.4746 - val_loss: 0.7272 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.8286 - acc: 0.3559 - val_loss: 0.7119 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.8120 - acc: 0.5678 - val_loss: 0.7035 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.7908 - acc: 0.4576 - val_loss: 0.6900 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.7726 - acc: 0.5678 - val_loss: 0.6773 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.7540 - acc: 0.5593 - val_loss: 0.6659 - val_acc: 0.5000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.7407 - acc: 0.7203 - val_loss: 0.6538 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.7184 - acc: 0.8390 - val_loss: 0.6442 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.7020 - acc: 0.8305 - val_loss: 0.6326 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.6859 - acc: 0.8814 - val_loss: 0.6258 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.6777 - acc: 0.9322 - val_loss: 0.6101 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.6572 - acc: 0.9237 - val_loss: 0.6025 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.6429 - acc: 0.9237 - val_loss: 0.6049 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.6432 - acc: 0.8898 - val_loss: 0.5992 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.6136 - acc: 0.9492 - val_loss: 0.5720 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.6013 - acc: 0.8729 - val_loss: 0.5681 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.5877 - acc: 0.9746 - val_loss: 0.5529 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.5586 - acc: 0.9407 - val_loss: 0.5472 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.5266 - acc: 0.9576 - val_loss: 0.5373 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4923 - acc: 0.9576 - val_loss: 0.5284 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4630 - acc: 0.9492 - val_loss: 0.5349 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4334 - acc: 0.9576 - val_loss: 0.5053 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4018 - acc: 0.9576 - val_loss: 0.5025 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.3738 - acc: 0.9661 - val_loss: 0.4669 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.3512 - acc: 0.9492 - val_loss: 0.4611 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.3257 - acc: 0.9492 - val_loss: 0.4433 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.3032 - acc: 0.9576 - val_loss: 0.4533 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.2888 - acc: 0.9407 - val_loss: 0.4684 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.2642 - acc: 0.9746 - val_loss: 0.4081 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.2526 - acc: 0.9576 - val_loss: 0.4509 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.2530 - acc: 0.9407 - val_loss: 0.4132 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.2251 - acc: 0.9576 - val_loss: 0.4229 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.2309 - acc: 0.9576 - val_loss: 0.3566 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.2093 - acc: 0.9661 - val_loss: 0.4033 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1987 - acc: 0.9576 - val_loss: 0.3819 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1929 - acc: 0.9576 - val_loss: 0.3500 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1855 - acc: 0.9746 - val_loss: 0.3635 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1797 - acc: 0.9746 - val_loss: 0.3144 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1740 - acc: 0.9831 - val_loss: 0.4317 - val_acc: 0.5000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1713 - acc: 0.9661 - val_loss: 0.3064 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.1745 - acc: 0.9576 - val_loss: 0.4128 - val_acc: 0.5000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1569 - acc: 0.9746 - val_loss: 0.3152 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.1524 - acc: 0.9576 - val_loss: 0.3182 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1595 - acc: 0.9407 - val_loss: 0.3056 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.1463 - acc: 0.9746 - val_loss: 0.3066 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.1414 - acc: 0.9576 - val_loss: 0.3091 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.1396 - acc: 0.9492 - val_loss: 0.2868 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1381 - acc: 0.9661 - val_loss: 0.2782 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.1331 - acc: 0.9576 - val_loss: 0.2973 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1289 - acc: 0.9576 - val_loss: 0.3086 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1284 - acc: 0.9576 - val_loss: 0.3078 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1280 - acc: 0.9576 - val_loss: 0.3561 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1253 - acc: 0.9576 - val_loss: 0.3083 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.1220 - acc: 0.9576 - val_loss: 0.3135 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.1195 - acc: 0.9576 - val_loss: 0.2664 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.1375 - acc: 0.9492 - val_loss: 0.2170 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.1175 - acc: 0.9661 - val_loss: 0.3005 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.1160 - acc: 0.9576 - val_loss: 0.3274 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.1152 - acc: 0.9661 - val_loss: 0.2485 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.1143 - acc: 0.9746 - val_loss: 0.2699 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.1098 - acc: 0.9661 - val_loss: 0.2572 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.1126 - acc: 0.9661 - val_loss: 0.2264 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.1210 - acc: 0.9661 - val_loss: 0.2069 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.1050 - acc: 0.9576 - val_loss: 0.2955 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.1047 - acc: 0.9746 - val_loss: 0.2672 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.1115 - acc: 0.9576 - val_loss: 0.2743 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.1236 - acc: 0.9407 - val_loss: 0.2141 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.1061 - acc: 0.9661 - val_loss: 0.2132 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.1048 - acc: 0.9492 - val_loss: 0.2090 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.1014 - acc: 0.9492 - val_loss: 0.2619 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.1030 - acc: 0.9661 - val_loss: 0.1888 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.1192 - acc: 0.9661 - val_loss: 0.4026 - val_acc: 0.5000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0983 - acc: 0.9746 - val_loss: 0.2368 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.1005 - acc: 0.9661 - val_loss: 0.1908 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0997 - acc: 0.9576 - val_loss: 0.2231 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0922 - acc: 0.9576 - val_loss: 0.2912 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0921 - acc: 0.9746 - val_loss: 0.2052 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0959 - acc: 0.9492 - val_loss: 0.3206 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.1025 - acc: 0.9576 - val_loss: 0.2627 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0977 - acc: 0.9746 - val_loss: 0.2343 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0887 - acc: 0.9576 - val_loss: 0.2850 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0884 - acc: 0.9746 - val_loss: 0.2275 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0896 - acc: 0.9576 - val_loss: 0.2778 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0899 - acc: 0.9661 - val_loss: 0.2007 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0869 - acc: 0.9576 - val_loss: 0.2734 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0924 - acc: 0.9746 - val_loss: 0.2042 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0858 - acc: 0.9661 - val_loss: 0.2133 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0879 - acc: 0.9661 - val_loss: 0.2265 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0830 - acc: 0.9576 - val_loss: 0.1744 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0850 - acc: 0.9576 - val_loss: 0.2343 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.1015 - acc: 0.9492 - val_loss: 0.1609 - val_acc: 1.0000\n",
            "2/2 [==============================] - 0s 536us/step\n",
            "30/30 [==============================] - 0s 55us/step\n",
            "Final test set loss: 0.160940\n",
            "Final test set accuracy: 1.000000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 2.2220 - acc: 0.2373 - val_loss: 1.9951 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.5589 - acc: 0.3390 - val_loss: 1.3412 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 1.1151 - acc: 0.4407 - val_loss: 0.9962 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9397 - acc: 0.5593 - val_loss: 0.9266 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.8820 - acc: 0.5254 - val_loss: 0.8846 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.8287 - acc: 0.5763 - val_loss: 0.8502 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.7851 - acc: 0.6441 - val_loss: 0.8441 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.7468 - acc: 0.6102 - val_loss: 0.8206 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.7082 - acc: 0.6695 - val_loss: 0.7793 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.6696 - acc: 0.6780 - val_loss: 0.7807 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.6381 - acc: 0.7034 - val_loss: 0.7464 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.6078 - acc: 0.7627 - val_loss: 0.7358 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.5691 - acc: 0.8305 - val_loss: 0.7460 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.5564 - acc: 0.8136 - val_loss: 0.6995 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.5277 - acc: 0.8051 - val_loss: 0.6899 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.5066 - acc: 0.8983 - val_loss: 0.6842 - val_acc: 0.5000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4919 - acc: 0.8983 - val_loss: 0.6564 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4712 - acc: 0.9068 - val_loss: 0.6581 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4469 - acc: 0.9068 - val_loss: 0.6429 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4336 - acc: 0.8983 - val_loss: 0.6331 - val_acc: 0.5000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4176 - acc: 0.9153 - val_loss: 0.6040 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4042 - acc: 0.9237 - val_loss: 0.5891 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.3888 - acc: 0.9322 - val_loss: 0.5817 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.3746 - acc: 0.9407 - val_loss: 0.5807 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.3557 - acc: 0.9237 - val_loss: 0.5541 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.3480 - acc: 0.9407 - val_loss: 0.5358 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.3302 - acc: 0.9322 - val_loss: 0.5262 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.3106 - acc: 0.9322 - val_loss: 0.5181 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.3074 - acc: 0.9492 - val_loss: 0.5051 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.3019 - acc: 0.9492 - val_loss: 0.5213 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.2893 - acc: 0.9492 - val_loss: 0.4815 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.2707 - acc: 0.9576 - val_loss: 0.4808 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.2735 - acc: 0.9492 - val_loss: 0.4695 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.2518 - acc: 0.9576 - val_loss: 0.4533 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.2510 - acc: 0.9407 - val_loss: 0.4488 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.2385 - acc: 0.9746 - val_loss: 0.5018 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.2245 - acc: 0.9492 - val_loss: 0.4360 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.2215 - acc: 0.9661 - val_loss: 0.4789 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.2231 - acc: 0.9576 - val_loss: 0.3978 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.2089 - acc: 0.9661 - val_loss: 0.4316 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.2090 - acc: 0.9492 - val_loss: 0.4010 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.2006 - acc: 0.9576 - val_loss: 0.3951 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.1993 - acc: 0.9407 - val_loss: 0.3971 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1954 - acc: 0.9407 - val_loss: 0.3947 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1784 - acc: 0.9661 - val_loss: 0.4800 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1863 - acc: 0.9492 - val_loss: 0.4321 - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1740 - acc: 0.9576 - val_loss: 0.3345 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1761 - acc: 0.9492 - val_loss: 0.3962 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1644 - acc: 0.9661 - val_loss: 0.4659 - val_acc: 0.5000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.1733 - acc: 0.9322 - val_loss: 0.3827 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1671 - acc: 0.9237 - val_loss: 0.3171 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.1570 - acc: 0.9576 - val_loss: 0.4024 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1612 - acc: 0.9407 - val_loss: 0.3143 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.1553 - acc: 0.9576 - val_loss: 0.3072 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.1511 - acc: 0.9322 - val_loss: 0.3533 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.1494 - acc: 0.9576 - val_loss: 0.3299 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1449 - acc: 0.9407 - val_loss: 0.3552 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.1479 - acc: 0.9492 - val_loss: 0.2894 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1424 - acc: 0.9407 - val_loss: 0.3208 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1398 - acc: 0.9576 - val_loss: 0.2550 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1395 - acc: 0.9576 - val_loss: 0.2521 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1343 - acc: 0.9492 - val_loss: 0.2438 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.1329 - acc: 0.9407 - val_loss: 0.2547 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.1337 - acc: 0.9661 - val_loss: 0.3491 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.1241 - acc: 0.9576 - val_loss: 0.2622 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.1261 - acc: 0.9746 - val_loss: 0.2774 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.1230 - acc: 0.9492 - val_loss: 0.3366 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.1267 - acc: 0.9492 - val_loss: 0.2674 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.1227 - acc: 0.9661 - val_loss: 0.2976 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.1146 - acc: 0.9576 - val_loss: 0.4125 - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.1217 - acc: 0.9407 - val_loss: 0.2206 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.1196 - acc: 0.9661 - val_loss: 0.3080 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.1173 - acc: 0.9661 - val_loss: 0.3782 - val_acc: 0.5000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.1152 - acc: 0.9661 - val_loss: 0.2506 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.1103 - acc: 0.9746 - val_loss: 0.3386 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.1180 - acc: 0.9576 - val_loss: 0.2730 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.1130 - acc: 0.9661 - val_loss: 0.1994 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.1190 - acc: 0.9492 - val_loss: 0.2160 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.1143 - acc: 0.9576 - val_loss: 0.2717 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.1141 - acc: 0.9576 - val_loss: 0.2370 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.1096 - acc: 0.9576 - val_loss: 0.2356 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.1105 - acc: 0.9492 - val_loss: 0.2837 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.1072 - acc: 0.9661 - val_loss: 0.2267 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.1049 - acc: 0.9661 - val_loss: 0.3139 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.1055 - acc: 0.9661 - val_loss: 0.1890 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.1043 - acc: 0.9661 - val_loss: 0.1613 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.1086 - acc: 0.9661 - val_loss: 0.2076 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.1079 - acc: 0.9576 - val_loss: 0.1769 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.1094 - acc: 0.9576 - val_loss: 0.1885 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.1046 - acc: 0.9661 - val_loss: 0.1614 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.1023 - acc: 0.9661 - val_loss: 0.3395 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.1063 - acc: 0.9831 - val_loss: 0.1751 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.1045 - acc: 0.9661 - val_loss: 0.1733 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.1004 - acc: 0.9661 - val_loss: 0.1420 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.1026 - acc: 0.9407 - val_loss: 0.1815 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.1040 - acc: 0.9492 - val_loss: 0.1508 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.1035 - acc: 0.9576 - val_loss: 0.2267 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0916 - acc: 0.9661 - val_loss: 0.3669 - val_acc: 0.5000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.1013 - acc: 0.9576 - val_loss: 0.2109 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0943 - acc: 0.9746 - val_loss: 0.2455 - val_acc: 1.0000\n",
            "2/2 [==============================] - 0s 1ms/step\n",
            "30/30 [==============================] - 0s 66us/step\n",
            "Final test set loss: 0.245479\n",
            "Final test set accuracy: 1.000000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.0230 - acc: 0.3644 - val_loss: 0.9238 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.8842 - acc: 0.7458 - val_loss: 0.8193 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.8049 - acc: 0.7712 - val_loss: 0.7913 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.7222 - acc: 0.8136 - val_loss: 0.7254 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.6524 - acc: 0.7797 - val_loss: 0.7184 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.5968 - acc: 0.8305 - val_loss: 0.7121 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.5520 - acc: 0.8051 - val_loss: 0.6857 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4876 - acc: 0.8136 - val_loss: 0.8138 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4900 - acc: 0.8136 - val_loss: 0.7220 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4612 - acc: 0.8220 - val_loss: 0.6106 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4359 - acc: 0.8475 - val_loss: 0.6035 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4014 - acc: 0.8644 - val_loss: 0.6704 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4375 - acc: 0.7458 - val_loss: 0.6005 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.3737 - acc: 0.8559 - val_loss: 0.6838 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.3507 - acc: 0.8983 - val_loss: 0.5552 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.3450 - acc: 0.9153 - val_loss: 0.5357 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.3186 - acc: 0.8814 - val_loss: 0.5225 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.3361 - acc: 0.8814 - val_loss: 0.6123 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.3107 - acc: 0.8898 - val_loss: 0.5767 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.2852 - acc: 0.9153 - val_loss: 0.5085 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.2873 - acc: 0.8644 - val_loss: 0.5195 - val_acc: 0.5000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.2936 - acc: 0.9068 - val_loss: 0.8638 - val_acc: 0.5000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.3192 - acc: 0.8644 - val_loss: 0.5384 - val_acc: 0.5000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.2459 - acc: 0.9407 - val_loss: 0.4652 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.2366 - acc: 0.9237 - val_loss: 0.4539 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.2405 - acc: 0.9237 - val_loss: 0.4221 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.2707 - acc: 0.8983 - val_loss: 0.6355 - val_acc: 0.5000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.2137 - acc: 0.9237 - val_loss: 0.4013 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.2285 - acc: 0.9068 - val_loss: 0.4180 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.1913 - acc: 0.9322 - val_loss: 0.4782 - val_acc: 0.5000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.2080 - acc: 0.9492 - val_loss: 0.3577 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.2038 - acc: 0.9322 - val_loss: 0.3912 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.2132 - acc: 0.9153 - val_loss: 0.7521 - val_acc: 0.5000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.2112 - acc: 0.9407 - val_loss: 0.3325 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.2741 - acc: 0.8983 - val_loss: 0.3476 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.2075 - acc: 0.9237 - val_loss: 0.8920 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.2248 - acc: 0.8983 - val_loss: 0.3098 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.1695 - acc: 0.9492 - val_loss: 1.0742 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.2206 - acc: 0.9153 - val_loss: 0.3250 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.2052 - acc: 0.9153 - val_loss: 0.3949 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.1740 - acc: 0.9322 - val_loss: 0.3988 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.1639 - acc: 0.9322 - val_loss: 0.2757 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.1690 - acc: 0.9576 - val_loss: 0.3544 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1447 - acc: 0.9576 - val_loss: 0.2799 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1542 - acc: 0.9492 - val_loss: 0.2546 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1653 - acc: 0.9322 - val_loss: 0.2582 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1637 - acc: 0.9322 - val_loss: 0.2492 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1665 - acc: 0.9153 - val_loss: 0.5399 - val_acc: 0.5000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1328 - acc: 0.9576 - val_loss: 0.5550 - val_acc: 0.5000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.1796 - acc: 0.8898 - val_loss: 0.5846 - val_acc: 0.5000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1336 - acc: 0.9407 - val_loss: 1.7907 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.2477 - acc: 0.8898 - val_loss: 0.2846 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1731 - acc: 0.9322 - val_loss: 0.5355 - val_acc: 0.5000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.1944 - acc: 0.9153 - val_loss: 0.2138 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.1405 - acc: 0.9407 - val_loss: 1.8554 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.1906 - acc: 0.9322 - val_loss: 0.2649 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1211 - acc: 0.9746 - val_loss: 0.2451 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.2547 - acc: 0.9068 - val_loss: 1.2424 - val_acc: 0.5000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1619 - acc: 0.9237 - val_loss: 1.3506 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1614 - acc: 0.9068 - val_loss: 1.1237 - val_acc: 0.5000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1539 - acc: 0.9492 - val_loss: 0.2031 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1099 - acc: 0.9746 - val_loss: 0.4133 - val_acc: 0.5000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.1571 - acc: 0.9322 - val_loss: 0.1790 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.1437 - acc: 0.9576 - val_loss: 1.4240 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.1717 - acc: 0.9153 - val_loss: 0.2343 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.1652 - acc: 0.9492 - val_loss: 0.2713 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.1399 - acc: 0.9492 - val_loss: 0.2069 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.1176 - acc: 0.9661 - val_loss: 0.8029 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.1768 - acc: 0.9407 - val_loss: 0.1767 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.1094 - acc: 0.9661 - val_loss: 0.2871 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.1452 - acc: 0.9492 - val_loss: 0.1579 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.1447 - acc: 0.9407 - val_loss: 0.6635 - val_acc: 0.5000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.1254 - acc: 0.9322 - val_loss: 0.1854 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.1537 - acc: 0.9407 - val_loss: 0.4304 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.3177 - acc: 0.8983 - val_loss: 0.8124 - val_acc: 0.5000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.1376 - acc: 0.9407 - val_loss: 0.3289 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.1263 - acc: 0.9322 - val_loss: 0.2436 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.1120 - acc: 0.9746 - val_loss: 0.2218 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.1575 - acc: 0.9492 - val_loss: 0.1918 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.1428 - acc: 0.9237 - val_loss: 0.3002 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.1335 - acc: 0.9492 - val_loss: 1.4985 - val_acc: 0.5000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.1843 - acc: 0.9407 - val_loss: 0.5055 - val_acc: 0.5000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.1364 - acc: 0.9407 - val_loss: 0.4637 - val_acc: 0.5000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.1450 - acc: 0.9492 - val_loss: 0.9025 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.1724 - acc: 0.9237 - val_loss: 0.3407 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.1881 - acc: 0.9068 - val_loss: 0.1385 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.2194 - acc: 0.8898 - val_loss: 0.1719 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.1506 - acc: 0.9407 - val_loss: 0.6401 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.1350 - acc: 0.9576 - val_loss: 0.3354 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.1159 - acc: 0.9661 - val_loss: 0.2558 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0931 - acc: 0.9661 - val_loss: 1.0546 - val_acc: 0.5000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.1305 - acc: 0.9237 - val_loss: 0.2109 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.1347 - acc: 0.9492 - val_loss: 0.1371 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0861 - acc: 0.9746 - val_loss: 0.1285 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.1104 - acc: 0.9576 - val_loss: 0.4854 - val_acc: 0.5000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.1459 - acc: 0.9322 - val_loss: 0.7401 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0953 - acc: 0.9576 - val_loss: 0.4692 - val_acc: 0.5000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.1131 - acc: 0.9492 - val_loss: 0.3027 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0979 - acc: 0.9661 - val_loss: 1.2256 - val_acc: 0.5000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.1348 - acc: 0.9407 - val_loss: 0.5131 - val_acc: 0.5000\n",
            "2/2 [==============================] - 0s 1ms/step\n",
            "30/30 [==============================] - 0s 82us/step\n",
            "Final test set loss: 0.513109\n",
            "Final test set accuracy: 0.500000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.7746 - acc: 0.3136 - val_loss: 1.3936 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.2013 - acc: 0.3898 - val_loss: 1.0923 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.9998 - acc: 0.6102 - val_loss: 0.9478 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.8752 - acc: 0.6271 - val_loss: 0.8759 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.7913 - acc: 0.6271 - val_loss: 0.8475 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.7447 - acc: 0.6271 - val_loss: 0.8192 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.7062 - acc: 0.6271 - val_loss: 0.8078 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.6746 - acc: 0.6271 - val_loss: 0.7823 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.6489 - acc: 0.6271 - val_loss: 0.7744 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.6265 - acc: 0.6271 - val_loss: 0.7558 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.6077 - acc: 0.6271 - val_loss: 0.7500 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.5909 - acc: 0.6441 - val_loss: 0.7370 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.5759 - acc: 0.6356 - val_loss: 0.7245 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.5627 - acc: 0.6610 - val_loss: 0.7189 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.5497 - acc: 0.6695 - val_loss: 0.7093 - val_acc: 0.5000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.5389 - acc: 0.6695 - val_loss: 0.7028 - val_acc: 0.5000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.5286 - acc: 0.6949 - val_loss: 0.6986 - val_acc: 0.5000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.5185 - acc: 0.7034 - val_loss: 0.6916 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.5099 - acc: 0.7119 - val_loss: 0.6889 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.5014 - acc: 0.7542 - val_loss: 0.6821 - val_acc: 0.5000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4945 - acc: 0.7712 - val_loss: 0.6802 - val_acc: 0.5000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4873 - acc: 0.7966 - val_loss: 0.6780 - val_acc: 0.5000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4804 - acc: 0.8136 - val_loss: 0.6737 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4747 - acc: 0.8390 - val_loss: 0.6729 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4694 - acc: 0.8814 - val_loss: 0.6666 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4628 - acc: 0.8559 - val_loss: 0.6646 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4578 - acc: 0.9153 - val_loss: 0.6587 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4532 - acc: 0.9322 - val_loss: 0.6538 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4482 - acc: 0.8983 - val_loss: 0.6517 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4444 - acc: 0.8898 - val_loss: 0.6501 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4395 - acc: 0.9492 - val_loss: 0.6450 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4360 - acc: 0.9153 - val_loss: 0.6441 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4321 - acc: 0.9492 - val_loss: 0.6399 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4275 - acc: 0.9153 - val_loss: 0.6386 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4246 - acc: 0.9492 - val_loss: 0.6357 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4205 - acc: 0.9407 - val_loss: 0.6355 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4169 - acc: 0.9576 - val_loss: 0.6338 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4129 - acc: 0.9576 - val_loss: 0.6300 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4098 - acc: 0.9576 - val_loss: 0.6272 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4074 - acc: 0.9576 - val_loss: 0.6238 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4036 - acc: 0.9576 - val_loss: 0.6237 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4006 - acc: 0.9576 - val_loss: 0.6227 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.3974 - acc: 0.9576 - val_loss: 0.6213 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.3951 - acc: 0.9576 - val_loss: 0.6196 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.3913 - acc: 0.9661 - val_loss: 0.6165 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.3891 - acc: 0.9576 - val_loss: 0.6121 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.3859 - acc: 0.9576 - val_loss: 0.6139 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.3832 - acc: 0.9746 - val_loss: 0.6101 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.3809 - acc: 0.9661 - val_loss: 0.6074 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.3774 - acc: 0.9661 - val_loss: 0.6067 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.3756 - acc: 0.9576 - val_loss: 0.6052 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.3723 - acc: 0.9661 - val_loss: 0.6039 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.3697 - acc: 0.9661 - val_loss: 0.6017 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.3671 - acc: 0.9746 - val_loss: 0.5971 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.3659 - acc: 0.9746 - val_loss: 0.5951 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.3632 - acc: 0.9661 - val_loss: 0.5930 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.3599 - acc: 0.9661 - val_loss: 0.5947 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.3576 - acc: 0.9746 - val_loss: 0.5916 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.3551 - acc: 0.9576 - val_loss: 0.5920 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.3541 - acc: 0.9746 - val_loss: 0.5912 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.3503 - acc: 0.9661 - val_loss: 0.5888 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.3490 - acc: 0.9746 - val_loss: 0.5872 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.3464 - acc: 0.9661 - val_loss: 0.5862 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.3436 - acc: 0.9746 - val_loss: 0.5810 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.3421 - acc: 0.9746 - val_loss: 0.5799 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.3400 - acc: 0.9746 - val_loss: 0.5751 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.3374 - acc: 0.9576 - val_loss: 0.5786 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.3358 - acc: 0.9746 - val_loss: 0.5759 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.3331 - acc: 0.9746 - val_loss: 0.5732 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.3316 - acc: 0.9661 - val_loss: 0.5728 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.3286 - acc: 0.9746 - val_loss: 0.5730 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.3271 - acc: 0.9746 - val_loss: 0.5688 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.3256 - acc: 0.9746 - val_loss: 0.5672 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.3227 - acc: 0.9746 - val_loss: 0.5644 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.3211 - acc: 0.9746 - val_loss: 0.5621 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.3196 - acc: 0.9746 - val_loss: 0.5628 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.3169 - acc: 0.9746 - val_loss: 0.5643 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.3154 - acc: 0.9746 - val_loss: 0.5624 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.3133 - acc: 0.9661 - val_loss: 0.5631 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.3119 - acc: 0.9661 - val_loss: 0.5581 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.3098 - acc: 0.9746 - val_loss: 0.5561 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.3079 - acc: 0.9746 - val_loss: 0.5571 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.3059 - acc: 0.9746 - val_loss: 0.5534 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.3040 - acc: 0.9746 - val_loss: 0.5535 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.3024 - acc: 0.9746 - val_loss: 0.5500 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.3007 - acc: 0.9746 - val_loss: 0.5476 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.2988 - acc: 0.9746 - val_loss: 0.5497 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.2977 - acc: 0.9746 - val_loss: 0.5469 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.2954 - acc: 0.9746 - val_loss: 0.5459 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.2935 - acc: 0.9746 - val_loss: 0.5441 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.2922 - acc: 0.9746 - val_loss: 0.5397 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.2902 - acc: 0.9746 - val_loss: 0.5394 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.2893 - acc: 0.9746 - val_loss: 0.5395 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.2872 - acc: 0.9746 - val_loss: 0.5368 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.2854 - acc: 0.9746 - val_loss: 0.5388 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.2844 - acc: 0.9746 - val_loss: 0.5374 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.2823 - acc: 0.9746 - val_loss: 0.5338 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.2809 - acc: 0.9746 - val_loss: 0.5310 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.2787 - acc: 0.9746 - val_loss: 0.5305 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.2777 - acc: 0.9661 - val_loss: 0.5273 - val_acc: 1.0000\n",
            "2/2 [==============================] - 0s 915us/step\n",
            "30/30 [==============================] - 0s 76us/step\n",
            "Final test set loss: 0.527298\n",
            "Final test set accuracy: 1.000000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.1839 - acc: 0.3136 - val_loss: 0.8996 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.0879 - acc: 0.3136 - val_loss: 0.8459 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 1.0286 - acc: 0.3051 - val_loss: 0.8557 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9735 - acc: 0.3475 - val_loss: 0.8532 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.9240 - acc: 0.3051 - val_loss: 0.8048 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.8757 - acc: 0.5678 - val_loss: 0.8041 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.8200 - acc: 0.6525 - val_loss: 0.7997 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.7482 - acc: 0.6525 - val_loss: 0.7772 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.6695 - acc: 0.6271 - val_loss: 0.7519 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.6087 - acc: 0.6186 - val_loss: 0.7247 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.5603 - acc: 0.6525 - val_loss: 0.7166 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.5230 - acc: 0.7627 - val_loss: 0.6908 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4815 - acc: 0.8729 - val_loss: 0.6700 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4506 - acc: 0.8475 - val_loss: 0.6621 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4338 - acc: 0.8559 - val_loss: 0.6318 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4006 - acc: 0.9237 - val_loss: 0.6026 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.3686 - acc: 0.9237 - val_loss: 0.6018 - val_acc: 0.5000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.3417 - acc: 0.9492 - val_loss: 0.6582 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.3440 - acc: 0.8983 - val_loss: 0.6288 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.3079 - acc: 0.9576 - val_loss: 0.5858 - val_acc: 0.5000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.2903 - acc: 0.9407 - val_loss: 0.4998 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.2713 - acc: 0.9576 - val_loss: 0.4923 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.2364 - acc: 0.9746 - val_loss: 0.6694 - val_acc: 0.5000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.2412 - acc: 0.9407 - val_loss: 0.4347 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.2220 - acc: 0.9576 - val_loss: 0.4257 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.2078 - acc: 0.9661 - val_loss: 0.4105 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.2000 - acc: 0.9746 - val_loss: 0.7148 - val_acc: 0.5000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.1895 - acc: 0.9407 - val_loss: 0.3886 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.1879 - acc: 0.9746 - val_loss: 0.3995 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.1743 - acc: 0.9661 - val_loss: 0.3545 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.1679 - acc: 0.9661 - val_loss: 0.4889 - val_acc: 0.5000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.1681 - acc: 0.9661 - val_loss: 0.4042 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.1673 - acc: 0.9407 - val_loss: 0.3243 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.1597 - acc: 0.9661 - val_loss: 0.3453 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.1526 - acc: 0.9661 - val_loss: 0.3405 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.1466 - acc: 0.9492 - val_loss: 0.3857 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.1491 - acc: 0.9746 - val_loss: 0.2973 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.1428 - acc: 0.9407 - val_loss: 0.3421 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.1448 - acc: 0.9746 - val_loss: 0.3423 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.1343 - acc: 0.9661 - val_loss: 0.4175 - val_acc: 0.5000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.1313 - acc: 0.9831 - val_loss: 0.3031 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.1323 - acc: 0.9746 - val_loss: 0.4113 - val_acc: 0.5000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.1289 - acc: 0.9746 - val_loss: 0.3267 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1250 - acc: 0.9746 - val_loss: 0.4340 - val_acc: 0.5000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1255 - acc: 0.9492 - val_loss: 0.4016 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1225 - acc: 0.9746 - val_loss: 0.3224 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1157 - acc: 0.9746 - val_loss: 0.4148 - val_acc: 0.5000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1233 - acc: 0.9661 - val_loss: 0.3310 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1182 - acc: 0.9746 - val_loss: 0.4138 - val_acc: 0.5000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.1129 - acc: 0.9915 - val_loss: 0.3785 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1175 - acc: 0.9746 - val_loss: 0.2527 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.1149 - acc: 0.9746 - val_loss: 0.3074 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1104 - acc: 0.9661 - val_loss: 0.3027 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.1118 - acc: 0.9746 - val_loss: 0.3273 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.1088 - acc: 0.9746 - val_loss: 0.2520 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0990 - acc: 0.9831 - val_loss: 0.5295 - val_acc: 0.5000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1089 - acc: 0.9576 - val_loss: 0.2689 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.1075 - acc: 0.9746 - val_loss: 0.2697 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1045 - acc: 0.9746 - val_loss: 0.2479 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1028 - acc: 0.9746 - val_loss: 0.3583 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1044 - acc: 0.9746 - val_loss: 0.2507 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1054 - acc: 0.9746 - val_loss: 0.3353 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0945 - acc: 0.9746 - val_loss: 0.2130 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0974 - acc: 0.9661 - val_loss: 0.3570 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0983 - acc: 0.9661 - val_loss: 0.3179 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0970 - acc: 0.9831 - val_loss: 0.4970 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.1065 - acc: 0.9661 - val_loss: 0.3354 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0999 - acc: 0.9746 - val_loss: 0.3021 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0993 - acc: 0.9661 - val_loss: 0.2501 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0981 - acc: 0.9746 - val_loss: 0.3441 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0930 - acc: 0.9831 - val_loss: 0.2553 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0957 - acc: 0.9661 - val_loss: 0.2047 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0956 - acc: 0.9746 - val_loss: 0.2626 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0927 - acc: 0.9746 - val_loss: 0.3889 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0927 - acc: 0.9831 - val_loss: 0.3315 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0920 - acc: 0.9746 - val_loss: 0.2404 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0899 - acc: 0.9746 - val_loss: 0.3539 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0933 - acc: 0.9746 - val_loss: 0.2556 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0929 - acc: 0.9661 - val_loss: 0.2478 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0921 - acc: 0.9661 - val_loss: 0.2525 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0921 - acc: 0.9746 - val_loss: 0.2725 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0886 - acc: 0.9661 - val_loss: 0.2344 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0906 - acc: 0.9831 - val_loss: 0.2854 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0887 - acc: 0.9831 - val_loss: 0.3838 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0846 - acc: 0.9746 - val_loss: 0.1891 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0920 - acc: 0.9831 - val_loss: 0.2178 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0928 - acc: 0.9661 - val_loss: 0.3813 - val_acc: 0.5000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0868 - acc: 0.9746 - val_loss: 0.3103 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0896 - acc: 0.9746 - val_loss: 0.2913 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0878 - acc: 0.9831 - val_loss: 0.2309 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0899 - acc: 0.9746 - val_loss: 0.3095 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0870 - acc: 0.9831 - val_loss: 0.3009 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0872 - acc: 0.9746 - val_loss: 0.2453 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0784 - acc: 0.9831 - val_loss: 0.5016 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0869 - acc: 0.9746 - val_loss: 0.3250 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0853 - acc: 0.9661 - val_loss: 0.3152 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0858 - acc: 0.9746 - val_loss: 0.2441 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0837 - acc: 0.9661 - val_loss: 0.2359 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0853 - acc: 0.9746 - val_loss: 0.2381 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0833 - acc: 0.9746 - val_loss: 0.4649 - val_acc: 0.5000\n",
            "2/2 [==============================] - 0s 746us/step\n",
            "30/30 [==============================] - 0s 59us/step\n",
            "Final test set loss: 0.464898\n",
            "Final test set accuracy: 0.500000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.7836 - acc: 0.3136 - val_loss: 1.4019 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.3007 - acc: 0.3136 - val_loss: 1.1120 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 1.0640 - acc: 0.3136 - val_loss: 0.9968 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9602 - acc: 0.5169 - val_loss: 0.9152 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.8647 - acc: 0.6780 - val_loss: 0.8691 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.8121 - acc: 0.7373 - val_loss: 0.8291 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.7715 - acc: 0.8051 - val_loss: 0.8007 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.7329 - acc: 0.8390 - val_loss: 0.7834 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.6971 - acc: 0.8390 - val_loss: 0.7581 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.6684 - acc: 0.8475 - val_loss: 0.7436 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.6388 - acc: 0.8729 - val_loss: 0.7294 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.6147 - acc: 0.8644 - val_loss: 0.7133 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.5948 - acc: 0.8729 - val_loss: 0.7032 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.5735 - acc: 0.8983 - val_loss: 0.6894 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.5569 - acc: 0.8814 - val_loss: 0.6821 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.5377 - acc: 0.8983 - val_loss: 0.6726 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.5233 - acc: 0.8814 - val_loss: 0.6621 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.5056 - acc: 0.9068 - val_loss: 0.6542 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4892 - acc: 0.8983 - val_loss: 0.6478 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4765 - acc: 0.9322 - val_loss: 0.6402 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4631 - acc: 0.9153 - val_loss: 0.6290 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4478 - acc: 0.9322 - val_loss: 0.6217 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4408 - acc: 0.9153 - val_loss: 0.6194 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4282 - acc: 0.9068 - val_loss: 0.6089 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4159 - acc: 0.9322 - val_loss: 0.6075 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4051 - acc: 0.9322 - val_loss: 0.5930 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.3925 - acc: 0.9237 - val_loss: 0.5910 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.3829 - acc: 0.9576 - val_loss: 0.5859 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.3796 - acc: 0.9153 - val_loss: 0.5829 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.3704 - acc: 0.9237 - val_loss: 0.5688 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.3556 - acc: 0.9576 - val_loss: 0.5690 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.3512 - acc: 0.9492 - val_loss: 0.5635 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.3437 - acc: 0.9492 - val_loss: 0.5550 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.3352 - acc: 0.9492 - val_loss: 0.5495 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.3277 - acc: 0.9576 - val_loss: 0.5454 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.3202 - acc: 0.9492 - val_loss: 0.5369 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.3151 - acc: 0.9492 - val_loss: 0.5364 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.3067 - acc: 0.9661 - val_loss: 0.5202 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.3050 - acc: 0.9407 - val_loss: 0.5181 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.2948 - acc: 0.9576 - val_loss: 0.5109 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.2891 - acc: 0.9492 - val_loss: 0.5152 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.2800 - acc: 0.9576 - val_loss: 0.5016 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.2755 - acc: 0.9661 - val_loss: 0.4924 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.2689 - acc: 0.9661 - val_loss: 0.4918 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.2645 - acc: 0.9576 - val_loss: 0.4842 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.2579 - acc: 0.9661 - val_loss: 0.4707 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.2510 - acc: 0.9661 - val_loss: 0.4744 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.2463 - acc: 0.9746 - val_loss: 0.4623 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.2410 - acc: 0.9661 - val_loss: 0.4543 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.2333 - acc: 0.9576 - val_loss: 0.4665 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.2304 - acc: 0.9492 - val_loss: 0.4345 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.2206 - acc: 0.9661 - val_loss: 0.4439 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.2136 - acc: 0.9661 - val_loss: 0.4205 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.2085 - acc: 0.9746 - val_loss: 0.4223 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.2058 - acc: 0.9407 - val_loss: 0.4165 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.1986 - acc: 0.9661 - val_loss: 0.3990 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1932 - acc: 0.9661 - val_loss: 0.4022 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.1922 - acc: 0.9661 - val_loss: 0.3961 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1902 - acc: 0.9576 - val_loss: 0.4038 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1883 - acc: 0.9661 - val_loss: 0.3752 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1771 - acc: 0.9661 - val_loss: 0.4059 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1716 - acc: 0.9661 - val_loss: 0.3679 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.1750 - acc: 0.9661 - val_loss: 0.3506 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.1691 - acc: 0.9661 - val_loss: 0.3745 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.1649 - acc: 0.9661 - val_loss: 0.3553 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.1624 - acc: 0.9661 - val_loss: 0.3588 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.1589 - acc: 0.9661 - val_loss: 0.3481 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.1658 - acc: 0.9576 - val_loss: 0.3553 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.1616 - acc: 0.9576 - val_loss: 0.3282 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.1635 - acc: 0.9407 - val_loss: 0.3795 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.1552 - acc: 0.9576 - val_loss: 0.3230 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.1464 - acc: 0.9746 - val_loss: 0.3463 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.1446 - acc: 0.9746 - val_loss: 0.3396 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.1433 - acc: 0.9746 - val_loss: 0.3382 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.1437 - acc: 0.9746 - val_loss: 0.3374 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.1406 - acc: 0.9746 - val_loss: 0.3157 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.1391 - acc: 0.9661 - val_loss: 0.3254 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.1385 - acc: 0.9746 - val_loss: 0.2986 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.1339 - acc: 0.9661 - val_loss: 0.3233 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.1344 - acc: 0.9576 - val_loss: 0.3139 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.1315 - acc: 0.9746 - val_loss: 0.3241 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.1293 - acc: 0.9576 - val_loss: 0.3139 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.1367 - acc: 0.9661 - val_loss: 0.3003 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.1293 - acc: 0.9661 - val_loss: 0.3287 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.1305 - acc: 0.9661 - val_loss: 0.2879 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.1239 - acc: 0.9661 - val_loss: 0.3268 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.1252 - acc: 0.9661 - val_loss: 0.3093 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.1264 - acc: 0.9661 - val_loss: 0.2627 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.1280 - acc: 0.9576 - val_loss: 0.3126 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.1228 - acc: 0.9746 - val_loss: 0.2782 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.1206 - acc: 0.9576 - val_loss: 0.2995 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.1177 - acc: 0.9661 - val_loss: 0.2850 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.1173 - acc: 0.9746 - val_loss: 0.2691 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.1158 - acc: 0.9746 - val_loss: 0.2904 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.1159 - acc: 0.9746 - val_loss: 0.2928 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.1156 - acc: 0.9661 - val_loss: 0.2859 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.1175 - acc: 0.9746 - val_loss: 0.2754 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.1133 - acc: 0.9661 - val_loss: 0.2832 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.1129 - acc: 0.9746 - val_loss: 0.2837 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.1163 - acc: 0.9746 - val_loss: 0.2693 - val_acc: 1.0000\n",
            "2/2 [==============================] - 0s 833us/step\n",
            "30/30 [==============================] - 0s 103us/step\n",
            "Final test set loss: 0.269322\n",
            "Final test set accuracy: 1.000000\n",
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.1122 - acc: 0.6864 - val_loss: 1.1140 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.9694 - acc: 0.6864 - val_loss: 1.0161 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.8067 - acc: 0.6864 - val_loss: 0.9480 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.6955 - acc: 0.6864 - val_loss: 0.8860 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.6132 - acc: 0.6864 - val_loss: 0.8370 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.5623 - acc: 0.6864 - val_loss: 0.8090 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.5259 - acc: 0.6864 - val_loss: 0.7489 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4968 - acc: 0.6864 - val_loss: 0.7313 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4665 - acc: 0.7797 - val_loss: 0.6934 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4357 - acc: 0.8898 - val_loss: 0.6894 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4023 - acc: 0.8644 - val_loss: 0.6526 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.3762 - acc: 0.9068 - val_loss: 0.6546 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.3473 - acc: 0.8983 - val_loss: 0.5994 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.3171 - acc: 0.9407 - val_loss: 0.5598 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.2998 - acc: 0.9492 - val_loss: 0.5695 - val_acc: 0.5000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.2777 - acc: 0.9322 - val_loss: 0.6013 - val_acc: 0.5000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.2581 - acc: 0.9407 - val_loss: 0.5907 - val_acc: 0.5000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.2441 - acc: 0.9407 - val_loss: 0.5232 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.2251 - acc: 0.9576 - val_loss: 0.5254 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.2118 - acc: 0.9576 - val_loss: 0.4362 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.2071 - acc: 0.9576 - val_loss: 0.4300 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.1916 - acc: 0.9661 - val_loss: 0.4301 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.1853 - acc: 0.9576 - val_loss: 0.4619 - val_acc: 0.5000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.1765 - acc: 0.9492 - val_loss: 0.5113 - val_acc: 0.5000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.1665 - acc: 0.9576 - val_loss: 0.5476 - val_acc: 0.5000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.1662 - acc: 0.9492 - val_loss: 0.4568 - val_acc: 0.5000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.1536 - acc: 0.9576 - val_loss: 0.4347 - val_acc: 0.5000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.1500 - acc: 0.9746 - val_loss: 0.3771 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.1529 - acc: 0.9576 - val_loss: 0.4753 - val_acc: 0.5000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.1415 - acc: 0.9492 - val_loss: 0.4409 - val_acc: 0.5000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.1377 - acc: 0.9661 - val_loss: 0.4638 - val_acc: 0.5000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.1341 - acc: 0.9322 - val_loss: 0.4186 - val_acc: 0.5000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.1351 - acc: 0.9576 - val_loss: 0.3434 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.1263 - acc: 0.9746 - val_loss: 0.3969 - val_acc: 0.5000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.1229 - acc: 0.9661 - val_loss: 0.4535 - val_acc: 0.5000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.1227 - acc: 0.9661 - val_loss: 0.4736 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.1202 - acc: 0.9576 - val_loss: 0.3490 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.1234 - acc: 0.9492 - val_loss: 0.2964 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.1147 - acc: 0.9661 - val_loss: 0.2828 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.1142 - acc: 0.9746 - val_loss: 0.2740 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.1155 - acc: 0.9661 - val_loss: 0.4488 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.1069 - acc: 0.9576 - val_loss: 0.2625 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.1113 - acc: 0.9576 - val_loss: 0.2632 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1079 - acc: 0.9576 - val_loss: 0.3782 - val_acc: 0.5000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1035 - acc: 0.9746 - val_loss: 0.5548 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1099 - acc: 0.9492 - val_loss: 0.3498 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1028 - acc: 0.9661 - val_loss: 0.4178 - val_acc: 0.5000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1059 - acc: 0.9576 - val_loss: 0.4247 - val_acc: 0.5000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1062 - acc: 0.9661 - val_loss: 0.2748 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.0978 - acc: 0.9661 - val_loss: 0.2426 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1036 - acc: 0.9576 - val_loss: 0.4150 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.0995 - acc: 0.9576 - val_loss: 0.2793 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1000 - acc: 0.9661 - val_loss: 0.3652 - val_acc: 0.5000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.0967 - acc: 0.9661 - val_loss: 0.2763 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.0929 - acc: 0.9746 - val_loss: 0.5366 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0948 - acc: 0.9492 - val_loss: 0.2482 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.0947 - acc: 0.9576 - val_loss: 0.3118 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.0930 - acc: 0.9746 - val_loss: 0.2826 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.0959 - acc: 0.9661 - val_loss: 0.5139 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.0895 - acc: 0.9576 - val_loss: 0.2298 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.0892 - acc: 0.9661 - val_loss: 0.2004 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.0927 - acc: 0.9746 - val_loss: 0.2146 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0859 - acc: 0.9746 - val_loss: 0.2270 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0882 - acc: 0.9746 - val_loss: 0.6657 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0963 - acc: 0.9576 - val_loss: 0.3704 - val_acc: 0.5000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0846 - acc: 0.9746 - val_loss: 0.5306 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0900 - acc: 0.9576 - val_loss: 0.3919 - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0881 - acc: 0.9576 - val_loss: 0.3089 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0853 - acc: 0.9661 - val_loss: 0.3623 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0876 - acc: 0.9576 - val_loss: 0.4511 - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0838 - acc: 0.9746 - val_loss: 0.3461 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0852 - acc: 0.9661 - val_loss: 0.3910 - val_acc: 0.5000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0825 - acc: 0.9576 - val_loss: 0.1696 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0909 - acc: 0.9661 - val_loss: 0.3747 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0853 - acc: 0.9576 - val_loss: 0.3159 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0837 - acc: 0.9661 - val_loss: 0.2369 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0796 - acc: 0.9661 - val_loss: 0.5257 - val_acc: 0.5000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0823 - acc: 0.9576 - val_loss: 0.1511 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0863 - acc: 0.9746 - val_loss: 0.3163 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0844 - acc: 0.9746 - val_loss: 0.3274 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0845 - acc: 0.9661 - val_loss: 0.2133 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0897 - acc: 0.9576 - val_loss: 0.1553 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0830 - acc: 0.9661 - val_loss: 0.3424 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0783 - acc: 0.9746 - val_loss: 0.4315 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0800 - acc: 0.9576 - val_loss: 0.2896 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0791 - acc: 0.9576 - val_loss: 0.3120 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0826 - acc: 0.9746 - val_loss: 0.3127 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0763 - acc: 0.9746 - val_loss: 0.1321 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0854 - acc: 0.9746 - val_loss: 0.3923 - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0829 - acc: 0.9661 - val_loss: 0.5332 - val_acc: 0.5000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0787 - acc: 0.9661 - val_loss: 0.2695 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0823 - acc: 0.9576 - val_loss: 0.3181 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0757 - acc: 0.9661 - val_loss: 0.2626 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0760 - acc: 0.9746 - val_loss: 0.4135 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0779 - acc: 0.9746 - val_loss: 0.2867 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0742 - acc: 0.9746 - val_loss: 0.5902 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0768 - acc: 0.9746 - val_loss: 0.3576 - val_acc: 0.5000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0724 - acc: 0.9831 - val_loss: 0.1036 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0879 - acc: 0.9746 - val_loss: 0.4310 - val_acc: 0.5000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0790 - acc: 0.9576 - val_loss: 0.2225 - val_acc: 1.0000\n",
            "2/2 [==============================] - 0s 584us/step\n",
            "30/30 [==============================] - 0s 57us/step\n",
            "Final test set loss: 0.222497\n",
            "Final test set accuracy: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNBXzekNw_3D",
        "colab_type": "code",
        "outputId": "d23d2328-5aa7-4215-dd01-6d96e1cf96ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "import pandas as pd\n",
        "print(\"Accuracy on test\")\n",
        "print(pd.DataFrame([final_acc]))\n",
        "print(\"Loss on test\")\n",
        "print(pd.DataFrame([final_loss]))\n",
        "print(\"Accuracy on CV\")\n",
        "print(pd.DataFrame([cv_acc]))\n",
        "print(\"Loss on CV\")\n",
        "print(pd.DataFrame([cv_loss]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test\n",
            "   adam  rms  sgd  ada  adelta  adamax  nadam\n",
            "0   1.0  1.0  0.5  1.0     0.5     1.0    1.0\n",
            "Loss on test\n",
            "      adam       rms       sgd       ada    adelta    adamax     nadam\n",
            "0  0.16094  0.245479  0.513109  0.527298  0.464898  0.269322  0.222497\n",
            "Accuracy on CV\n",
            "   adam  rms       sgd  ada    adelta  adamax  nadam\n",
            "0   1.0  1.0  0.966667  1.0  0.966667     1.0    1.0\n",
            "Loss on CV\n",
            "       adam       rms       sgd       ada   adelta    adamax    nadam\n",
            "0  0.027141  0.036026  0.067701  0.194133  0.05761  0.040663  0.02346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2BMfE3k5rot",
        "colab_type": "text"
      },
      "source": [
        "**Adam is choosen as the best model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2CnQ_KK9LQB",
        "colab_type": "code",
        "outputId": "89b92e5f-d917-4288-f356-a9600a00b562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Hyperparameters\n",
        "#optimizer-adam\n",
        "#activation- relu, softmax\n",
        "#loss='categorical_crossentropy'\n",
        "#epoches =100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7ff720b556a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVaYb3_7w__H",
        "colab_type": "code",
        "outputId": "87bdb55a-37d1-4b99-84aa-a274aab76d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "history=model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=100, validation_data=(test_x, test_y))\n",
        "# Test on unseen data\n",
        "results_cv = model.evaluate(X_val, y_val)\n",
        "accuracy_=results_cv[1]\n",
        "loss_=results_cv[0]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 118 samples, validate on 2 samples\n",
            "Epoch 1/100\n",
            " - 3s - loss: 1.1478 - acc: 0.0508 - val_loss: 1.1165 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.9956 - acc: 0.2712 - val_loss: 1.0746 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.9500 - acc: 0.4915 - val_loss: 1.0739 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9243 - acc: 0.5593 - val_loss: 1.0696 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.9004 - acc: 0.6525 - val_loss: 1.0507 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.8746 - acc: 0.6949 - val_loss: 1.0563 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.8486 - acc: 0.6949 - val_loss: 1.0439 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.8236 - acc: 0.7458 - val_loss: 1.0255 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.7947 - acc: 0.8390 - val_loss: 1.0202 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.7563 - acc: 0.8136 - val_loss: 0.9940 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.7250 - acc: 0.8051 - val_loss: 0.9625 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.6971 - acc: 0.9068 - val_loss: 0.9500 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.6695 - acc: 0.8729 - val_loss: 0.9275 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.6290 - acc: 0.9407 - val_loss: 0.8741 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.5760 - acc: 0.9237 - val_loss: 0.8361 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.5463 - acc: 0.9492 - val_loss: 0.7928 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.5120 - acc: 0.8983 - val_loss: 0.7613 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4781 - acc: 0.9322 - val_loss: 0.7400 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4491 - acc: 0.9322 - val_loss: 0.7040 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4214 - acc: 0.9322 - val_loss: 0.6828 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4015 - acc: 0.9661 - val_loss: 0.6493 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.3746 - acc: 0.9407 - val_loss: 0.6086 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.3501 - acc: 0.9407 - val_loss: 0.5955 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.3206 - acc: 0.9576 - val_loss: 0.5573 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.3007 - acc: 0.9576 - val_loss: 0.5186 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.2818 - acc: 0.9576 - val_loss: 0.5281 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.2617 - acc: 0.9746 - val_loss: 0.4712 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.2455 - acc: 0.9661 - val_loss: 0.4649 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.2262 - acc: 0.9661 - val_loss: 0.4339 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.2158 - acc: 0.9661 - val_loss: 0.4116 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.2021 - acc: 0.9746 - val_loss: 0.3820 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.1914 - acc: 0.9661 - val_loss: 0.4042 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.1819 - acc: 0.9746 - val_loss: 0.3867 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.1801 - acc: 0.9661 - val_loss: 0.4031 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.1704 - acc: 0.9746 - val_loss: 0.3631 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.1615 - acc: 0.9746 - val_loss: 0.3676 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.1572 - acc: 0.9661 - val_loss: 0.3294 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.1499 - acc: 0.9661 - val_loss: 0.3198 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.1449 - acc: 0.9831 - val_loss: 0.3417 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.1468 - acc: 0.9661 - val_loss: 0.3385 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.1418 - acc: 0.9661 - val_loss: 0.3462 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.1352 - acc: 0.9831 - val_loss: 0.3130 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.1389 - acc: 0.9746 - val_loss: 0.3081 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.1353 - acc: 0.9492 - val_loss: 0.2638 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.1334 - acc: 0.9746 - val_loss: 0.3070 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.1261 - acc: 0.9746 - val_loss: 0.3217 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.1294 - acc: 0.9576 - val_loss: 0.2995 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.1346 - acc: 0.9492 - val_loss: 0.2454 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.1307 - acc: 0.9576 - val_loss: 0.2707 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.1263 - acc: 0.9661 - val_loss: 0.2719 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.1203 - acc: 0.9746 - val_loss: 0.2634 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.1245 - acc: 0.9576 - val_loss: 0.2705 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.1120 - acc: 0.9831 - val_loss: 0.2679 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.1167 - acc: 0.9661 - val_loss: 0.2853 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.1130 - acc: 0.9576 - val_loss: 0.2560 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.1075 - acc: 0.9746 - val_loss: 0.2538 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.1039 - acc: 0.9831 - val_loss: 0.2639 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.1083 - acc: 0.9746 - val_loss: 0.2395 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.1024 - acc: 0.9746 - val_loss: 0.2707 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.1015 - acc: 0.9746 - val_loss: 0.2563 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.1028 - acc: 0.9746 - val_loss: 0.2724 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.1026 - acc: 0.9746 - val_loss: 0.2291 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.1037 - acc: 0.9661 - val_loss: 0.2045 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.1004 - acc: 0.9661 - val_loss: 0.2502 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.1034 - acc: 0.9492 - val_loss: 0.2412 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0978 - acc: 0.9746 - val_loss: 0.2308 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0951 - acc: 0.9746 - val_loss: 0.2207 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.1039 - acc: 0.9576 - val_loss: 0.1618 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.1004 - acc: 0.9746 - val_loss: 0.2704 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0953 - acc: 0.9661 - val_loss: 0.2260 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0941 - acc: 0.9661 - val_loss: 0.2221 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0892 - acc: 0.9746 - val_loss: 0.2571 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0950 - acc: 0.9831 - val_loss: 0.1961 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0996 - acc: 0.9492 - val_loss: 0.1735 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0910 - acc: 0.9746 - val_loss: 0.2250 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0912 - acc: 0.9831 - val_loss: 0.1875 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0864 - acc: 0.9831 - val_loss: 0.2320 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0891 - acc: 0.9831 - val_loss: 0.2188 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0921 - acc: 0.9661 - val_loss: 0.1795 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0937 - acc: 0.9746 - val_loss: 0.2132 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0889 - acc: 0.9831 - val_loss: 0.1937 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0839 - acc: 0.9746 - val_loss: 0.2380 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0865 - acc: 0.9661 - val_loss: 0.1694 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0880 - acc: 0.9746 - val_loss: 0.2620 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0817 - acc: 0.9831 - val_loss: 0.1795 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0839 - acc: 0.9661 - val_loss: 0.2505 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0800 - acc: 0.9831 - val_loss: 0.1776 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0872 - acc: 0.9831 - val_loss: 0.1774 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0880 - acc: 0.9661 - val_loss: 0.2156 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0851 - acc: 0.9661 - val_loss: 0.1850 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0814 - acc: 0.9661 - val_loss: 0.2070 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0879 - acc: 0.9746 - val_loss: 0.2652 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0836 - acc: 0.9746 - val_loss: 0.1964 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0817 - acc: 0.9746 - val_loss: 0.1591 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0816 - acc: 0.9746 - val_loss: 0.2131 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0821 - acc: 0.9831 - val_loss: 0.2067 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0820 - acc: 0.9746 - val_loss: 0.1908 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0822 - acc: 0.9831 - val_loss: 0.1820 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0939 - acc: 0.9492 - val_loss: 0.1980 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0915 - acc: 0.9661 - val_loss: 0.1838 - val_acc: 1.0000\n",
            "30/30 [==============================] - 0s 89us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC7NlIxLxAC2",
        "colab_type": "code",
        "outputId": "3a04fd84-0067-488e-f75f-7739dbc4262e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Summary of neural network\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_213 (Dense)            (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 193\n",
            "Trainable params: 193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzt5rDi_w_9S",
        "colab_type": "code",
        "outputId": "94f7a205-c97c-4635-e647-9e95ec8944b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "# Output network visualization\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"352pt\" viewBox=\"0.00 0.00 196.00 264.00\" width=\"261pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 260)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 192,-260 192,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140699373977104 -->\n<g class=\"node\" id=\"node1\">\n<title>140699373977104</title>\n<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 188,-255.5 188,-219.5 0,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-233.8\">dense_213_input: InputLayer</text>\n</g>\n<!-- 140699373976152 -->\n<g class=\"node\" id=\"node2\">\n<title>140699373976152</title>\n<polygon fill=\"none\" points=\"33,-146.5 33,-182.5 155,-182.5 155,-146.5 33,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-160.8\">dense_213: Dense</text>\n</g>\n<!-- 140699373977104&#45;&gt;140699373976152 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140699373977104-&gt;140699373976152</title>\n<path d=\"M94,-219.4551C94,-211.3828 94,-201.6764 94,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"97.5001,-192.5903 94,-182.5904 90.5001,-192.5904 97.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140699373976656 -->\n<g class=\"node\" id=\"node3\">\n<title>140699373976656</title>\n<polygon fill=\"none\" points=\"33,-73.5 33,-109.5 155,-109.5 155,-73.5 33,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-87.8\">dense_214: Dense</text>\n</g>\n<!-- 140699373976152&#45;&gt;140699373976656 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140699373976152-&gt;140699373976656</title>\n<path d=\"M94,-146.4551C94,-138.3828 94,-128.6764 94,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"97.5001,-119.5903 94,-109.5904 90.5001,-119.5904 97.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140699365502648 -->\n<g class=\"node\" id=\"node4\">\n<title>140699365502648</title>\n<polygon fill=\"none\" points=\"33,-.5 33,-36.5 155,-36.5 155,-.5 33,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-14.8\">dense_215: Dense</text>\n</g>\n<!-- 140699373976656&#45;&gt;140699365502648 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140699373976656-&gt;140699365502648</title>\n<path d=\"M94,-73.4551C94,-65.3828 94,-55.6764 94,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"97.5001,-46.5903 94,-36.5904 90.5001,-46.5904 97.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmQ7b5Qk7e5l",
        "colab_type": "code",
        "outputId": "4fd732f6-e892-47f2-d4bc-c3b538ba9449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx1kgMMYYj/v/HScYk79ItN8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWvFMxb7fOV",
        "colab_type": "code",
        "outputId": "fe194cb6-f6d7-4bbe-a170-7e3549239763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn9NAoJRSfk4DgVJK+TkNBEop9f8b4QAAgvlxOd3qrBUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}